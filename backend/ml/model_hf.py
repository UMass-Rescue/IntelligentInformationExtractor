# -*- coding: utf-8 -*-
"""Copy of working(fast) of Chat_with_MultiplePDFs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1itJLR9nJt0Yip5HsvUeKcYyqWW_m6JJw
"""

# !pip install langchain
# !pip install torch
# !pip install faiss-cpu
# !pip install huggingface-hub
# !pip install pypdf
# !pip -q install accelerate
# !pip install llama-cpp-python
# !pip -q install git+https://github.com/huggingface/transformers
# !pip install InstructorEmbedding
# !pip install sentence-transformers==2.2.2
# !pip install python-dotenv==1.0.0
# !pip install --upgrade numpy
# !pip install unstructured
# !pip install transformers
# !pip install sentencepiece
# !pip install accelerate
# !pip install --upgrade --quiet huggingface_hub
# !pip install pdfplumber

from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.llms import LlamaCpp
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFDirectoryLoader,DirectoryLoader,PDFPlumberLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain.llms import HuggingFaceHub
from dotenv import load_dotenv
import sys
import os
from getpass import getpass
from langchain_community.llms import HuggingFaceEndpoint
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate



def load_pdf(path):
    #load pdf files
    pdf_loader = PyPDFLoader(path)

    # Initialize an empty list to hold the data
    data = []
    pdf_documents = pdf_loader.load()
    print(pdf_documents)
    print(type(pdf_documents))
    if pdf_documents:
        data += pdf_documents

    # If no supported files are found, exit
    if not data:
        print("No supported file types found.")
        sys.exit()
    return data


# print(data)
# type(pdf_documents)

#Step 05: Split the Extracted Data into Text Chunks
def get_text_chunks(data):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)
    text_chunks = text_splitter.split_documents(data)
    # len(text_chunks)
    # #get the third chunk
    # text_chunks[2]
    return text_chunks

#Step 06:Downlaod the Embeddings
def get_vectorstore(text_chunks):
    # embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")
    #Step 08: Create Embeddings for each of the Text Chunk
    vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)
    return vector_store

def process_file(file, selected_categories):
    data = load_pdf(file)
    #get text chunks
    text_chunks = get_text_chunks(data)
    #create vector store
    vector_store = get_vectorstore(text_chunks)

    #Import Model
    # llm = LlamaCpp(
    #     streaming = True,
    #     model_path="/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    #     temperature=0.75,
    #     top_p=1,
    #     verbose=True,
    #     n_ctx=4096
    # )

    HUGGINGFACEHUB_API_TOKEN = getpass()
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

    llm = HuggingFaceEndpoint(
        repo_id="mistralai/Mistral-7B-Instruct-v0.2", max_length=512, temperature=0.5, token=HUGGINGFACEHUB_API_TOKEN
    )
    template = """Question: {query}

        Answer: """
    prompt = PromptTemplate.from_template(template)
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=vector_store.as_retriever(search_kwargs={"k": 2}))

    category_prompts = {
        "Case Details": ["Who were the main perpetrators of child abuse ?", "What led to the discovery of the abuse?","What was the motivation behind the abuse ?"],
        "Missing child information": ["What were the specific acts of abuse committed against the children?", "Identify any specific city or locations  where the abuse took place.","What was the child's age who was abused?","Identify the date, time, and location where the child was last seen."],
        "Possible Abductor Information": ["Identify any mentions of the perpetrators' professions or occupations in the document.", "Note any mentions of the perpetrators' relationships or dynamics with the victims.","Where are the perpetrators from?"],
        "Sentencing and Legal Proceedings": ["Extract the sentences or potential sentences for the perpetrators in brief."],
        "Medical Information": ["Note any medications or medical treatments the child went through post abuse as mentioned in the data", "Describe any physical disabilities, cognitive impairments, or special needs of the child that may affect their well-being or ability to seek help"],
        "Witness Counts": ["Provide details of eyewitness testimonies related to the case mentioned in the document."]
    }

    data = []
    for selected_category in selected_categories:
        category_output = {"category": selected_category, "output": {}}
        for prompt in category_prompts[selected_category]:
            try:
                result = qa.run(prompt)
                category_output["output"][prompt] = result
            except Exception as e:
                print(f"Error processing prompt '{prompt}': {e}")
                category_output["output"][prompt] = "Error processing prompt"

        data.append(category_output)
    return data


# Example usage
# if __name__ == "__main__":

#     FILE_PATH = "./data/story.pdf"

#     #load the pdf
#     data = load_pdf(FILE_PATH)

#     #get text chunks
#     text_chunks = get_text_chunks(data)

#     #create vector store
#     vector_store = get_vectorstore(text_chunks)

#     #Import Model
#     # llm = LlamaCpp(
#     #     streaming = True,
#     #     model_path="/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
#     #     temperature=0.75,
#     #     top_p=1,
#     #     verbose=True,
#     #     n_ctx=4096
#     # )

#     HUGGINGFACEHUB_API_TOKEN = getpass()
#     os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

#     llm = HuggingFaceEndpoint(
#         repo_id="mistralai/Mistral-7B-Instruct-v0.2", max_length=512, temperature=0.5, token=HUGGINGFACEHUB_API_TOKEN
#     )

#     query = "Who abused the child in the document?"

#     template = """Question: {query}

#         Answer: """

#     prompt = PromptTemplate.from_template(template)

#     qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=vector_store.as_retriever(search_kwargs={"k": 2}))

#     qa.run(query)


#     while True:
#       selected_category = input("Enter the category you want to process: ")
#       if selected_category == 'exit':
#         print('Exiting')
#         sys.exit()
#       if selected_category == '':
#         continue
#       result = process_file(selected_category, qa)
#       if result:
#         print(f"Category: {result['category']}")
#         for prompt, output in result["output"].items():
#             print(f"Prompt: {prompt}")
#             print(f"Output: {output}")
#             print("-" * 20)

